---
title: "ReporteRegresiónLogistica"
author: "Flavio Galán, José Prince"
date: "`r Sys.Date()`"
output: html_document
---

Link del repositorio: https://github.com/ElrohirGT/Proyecto2_MineriaDeDatos

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

library(class)
library(caret)
library(dplyr)
library(Metrics)
library(profvis)
randomSeed <- 123

# Cargar datos
training_data <- read.csv("data/train.csv")

# Reemplazar valores NA con 0
training_data[is.na(training_data)] <- 0

mean_price <- mean(training_data$SalePrice, na.rm = TRUE)
sd_price <- sd(training_data$SalePrice, na.rm = TRUE)

# Definir los límites
lower_limit <- mean_price - (1 * sd_price)
upper_limit <- mean_price + (2 * sd_price)

# Crear la variable de clasificación
training_data$Category <- ifelse(training_data$SalePrice < lower_limit, "Baratas",
                                ifelse(training_data$SalePrice > upper_limit, "Caras", "Medianas"))
training_data <- training_data %>%
  mutate(
    EsBarata = ifelse(SalePrice < lower_limit, "Sí", "No"),
    EsCara = ifelse(SalePrice > upper_limit, "Sí", "No"),
    EsMediana = ifelse(SalePrice >= lower_limit & SalePrice <= upper_limit, "Sí", "No")
  )

training_data$Category <- as.factor(training_data$Category)

predictors <- training_data %>% select(-Category)
response <- training_data$Category

# Normalización (Estandarización Z-score)
preProc <- preProcess(predictors, method = c("center", "scale"))
predictors_scaled <- predict(preProc, predictors)

# Combinar las predictoras escaladas con la variable de respuesta
scaled_data <- cbind(predictors_scaled, Category = response)
scaled_data <- as.data.frame(scaled_data) # Convertir a data.frame

# Semilla para reproducibilidad
set.seed(randomSeed)

# Separar datos en entrenamiento (70%) y verificación (30%)
train_indices <- createDataPartition(scaled_data$Category, p = 0.7, list = FALSE)

train_data <- scaled_data[train_indices, ]
test_data <- scaled_data[-train_indices, ]
```

# Modelo de Regresión Logística

Debido a que ya tenemos los datos por las entregas anteriores procedemos a elaborar un modelo de regresión logística para la variable "EsCara" utilizando validación cruzada.

```{r Modelo de Regresión Logística}
# install.packages("caret")
# install.packages("e1071")  # requerido por caret para modelos SVM y otros

library(caret)
library(e1071)

set.seed(randomSeed)  # Para reproducibilidad

control <- trainControl(method = "cv",    # cross-validation
                        number = 10,      # número de folds
                        classProbs = TRUE,  # para clasificación
                        summaryFunction = twoClassSummary)  # para métricas como ROC

# Puedes cambiar a Accuracy, Sensitivity, etc.
#modeloBarata <- train(EsBarata ~ LotArea + OverallQual + YearBuilt + GarageCars + GrLivArea,
#                   data = train_data,
#                   method = "glm",
#                   family = "binomial",
#                   trControl = control,
#                   metric = "ROC")
#modeloMediana <- train(EsBarata ~ LotArea + OverallQual + YearBuilt + GarageCars + GrLivArea,
#                   data = train_data,
#                   method = "glm",
#                   family = "binomial",
#                   trControl = control,
#                   metric = "ROC")
modeloCara <- train(EsCara ~ LotArea + OverallQual + YearBuilt + GarageCars + GrLivArea,
                   data = train_data,
                   method = "glm",
                   family = "binomial",
                   trControl = control,
                   metric = "Accuracy")

#print("Modelo de EsBarata")
#print(modeloBarata)

#print("Modelo de EsMediana")
#print(modeloMediana)

print("Modelo de EsCara")
summary(modeloCara)
```

Analizando los coeficientes podemos ver que varias de las variables utilizadas tienen un p-value menor a 0.05, pero no son todas, LotArea y YearBuilt tienen un p value demasiado elevado, lo que me lleva a pensar que realmente no necesariamente se correlacionan con el precio de la vivienda.

Utilizando el modelo con el conjunto de verificación podemos ver que:
```{r Validación del Modelo}
predicciones <- predict(modeloCara, newdata = test_data)

# Convertir a clases (0 o 1) usando un umbral (por ejemplo, 0.5)
# predicciones <- ifelse(probabilidades > 0.5, 1, 0)
confusionMatrix(as.factor(predicciones), as.factor(test_data$EsCara))

```

Viendo los resultados del modelo, podemos ver que aunque el "Accuracy" es muy alto, realmente el modelo es medio malo, ya que nuestro Balanced Accuracy apenas llega a 77%, esto se debe a que realmente la cantidad de casas que cumplen nuestra definición de "cara" es extremadamente alta (2 desviaciónes estándar por encima de la media). Por lo tanto tenemos que balancear la muestra para que el modelo pueda aprender características sobre este conjunto de datos reducido.

```{r Modelo modificado}

# install.packages("ROSE")
library(ROSE)

# Cargar datos
training_data <- read.csv("data/train.csv")

# Reemplazar valores NA con 0
training_data[is.na(training_data)] <- 0

mean_price <- mean(training_data$SalePrice, na.rm = TRUE)
sd_price <- sd(training_data$SalePrice, na.rm = TRUE)

# Definir los límites
lower_limit <- mean_price - (1 * sd_price)
upper_limit <- mean_price + (2 * sd_price)


# Crear la variable de clasificación
training_data$Category <- ifelse(training_data$SalePrice < lower_limit, "Baratas",
                           ifelse(training_data$SalePrice > upper_limit, "Caras", "Medianas"))
training_data <- training_data %>%
  mutate(
    EsBarata = ifelse(SalePrice < lower_limit, "Sí", "No"),
    EsCara = ifelse(SalePrice > upper_limit, "Sí", "No"),
    EsMediana = ifelse(SalePrice >= lower_limit & SalePrice <= upper_limit, "Sí", "No")
  )
training_data$Category <- as.factor(training_data$Category)
balanced_data <- ROSE(EsCara ~   OverallQual  + GarageCars + GrLivArea + Category, data = training_data)$data

predictors <- balanced_data %>% select(-Category)
response <- balanced_data$Category

# Normalización (Estandarización Z-score)
preProc <- preProcess(predictors, method = c("center", "scale"))
predictors_scaled <- predict(preProc, predictors)

# Semilla para reproducibilidad
set.seed(randomSeed) 

# Separar datos en entrenamiento (70%) y verificación (30%)
train_indices <- createDataPartition(response, p = 0.7, list = FALSE)

train_data <- predictors_scaled[train_indices, ]
test_data <- predictors_scaled[-train_indices, ]

library(caret)
library(e1071)

set.seed(randomSeed)  # Para reproducibilidad

control <- trainControl(method = "cv",    # cross-validation
                        number = 10,      # número de folds
                        classProbs = TRUE,  # para clasificación
                        summaryFunction = twoClassSummary)  # para métricas como ROC

# Puedes cambiar a Accuracy, Sensitivity, etc.
#modeloBarata <- train(EsBarata ~ LotArea + OverallQual + YearBuilt + GarageCars + GrLivArea,
#                   data = train_data,
#                   method = "glm",
#                   family = "binomial",
#                   trControl = control,
#                   metric = "ROC")
#modeloMediana <- train(EsBarata ~ LotArea + OverallQual + YearBuilt + GarageCars + GrLivArea,
#                   data = train_data,
#                   method = "glm",
#                   family = "binomial",
#                   trControl = control,
#                   metric = "ROC")
modeloMejoradoCara <- train(EsCara ~ OverallQual  + GarageCars + GrLivArea,
                   data = train_data,
                   method = "glm",
                   family = "binomial",
                   trControl = control,
                   metric = "Precision-Recall AUC")

#print("Modelo de EsBarata")
#print(modeloBarata)

#print("Modelo de EsMediana")
#print(modeloMediana)

print("Modelo de EsCara")

predicciones <- predict(modeloMejoradoCara, newdata = test_data)

# Convertir a clases (0 o 1) usando un umbral (por ejemplo, 0.5)
# predicciones <- ifelse(probabilidades > 0.5, 1, 0)
confusionMatrix(as.factor(predicciones), as.factor(test_data$EsCara))
```

¡Podemos ver que este modelo se comporta de una manera mucho mejor al anterior! Aunque el accuracy normal disminuyó considerablemente el accuracy balanceado aumentó a 92%! Esto se debe principalmente a que nuestro modelo ya es capaz de identificar muchas más casas que sí son consideradas caras, pueso que las incluimos más seguido dentro del dataset.

## Análisis de Overfitting/Underfitting

¡Para analizar el overfitting/underfitting del modelo necesitamos evaluarlo con los datos de entrenamiento y comparar sus resultados con respecto a los datos de verificación!

```{r Análisis de Overfitting}
library(plotly)

print("Usando data de entrenamiento")
predicciones <- predict(modeloMejoradoCara, newdata = train_data)

# Convertir a clases (0 o 1) usando un umbral (por ejemplo, 0.5)
# predicciones <- ifelse(probabilidades > 0.5, 1, 0)
# confusionMatrix(as.factor(predicciones), as.factor(train_data$EsCara))


curva <- learning_curve_dat(dat = train_data,
                            outcome = "EsCara",
                            proportion = seq(0.1, 1.0, by = 0.1),
                            test_prop = 0.3,
                            method = "glm",
                            metric = "Accuracy",
                            family = "binomial")

# Graficar la curva de aprendizaje
ggplotly(
  ggplot(curva, aes(x = Training_Size, y = Accuracy, color = Data)) +
  geom_smooth(se = FALSE) +
  labs(title = "Curva de Aprendizaje - Regresión Logística",
       y = "Accuracy", x = "Tamaño del conjunto de entrenamiento")
)
```

Como podemos ver, las curvas tanto de validación como de entrenamiento se siguen muy cercanamente en los rangos cercanos a 300 datos y de 500 en adelante. En el final, aunque no convergen se puede ver que sí se encuentran muy cercanos entre sí por lo tanto no hay Overfitting. Tampoco creemos que haya underfitting, puesto que aunque sí están juntas la mayoría del tiempo, el valor de accuracy es demasiado alto (mayor a 94%). Por lo que consideramos que el modelo realmente sí aprendió de forma correcta luego de aplicarle un resampling a los datos de entrada para que no tuviera despreciara minorías.

## Eficiencia del modelo

Es importante determinar como podemos mejorar nuestro modelo y esto se puede hacer modificando los hiperparametros usados. En la implementación actual del modelo de regresión logística utilizando el método "glm" dentro de la función **train** de la librería **caret**, no se puede realizar ajustes automáticos de hiperparámetros. Esto se debe a que la función **glm** en R, implementa la regresión logísitca estándar, no posee hiperparámetros intrínsecos que puedan ser optimizados. 

En este caso el único "tunning" que se podría hacer es volver a realizar la ingeniería de caractrísticas y cambiar la selección de las variables predictoras pero esto involucraría cambiar el resto de los modelos que se hicieron en entregas anteriores por lo que no se procedera con este método y se dejara el tunning estandar que ofrece el modelo.

```{r model eficiency}
# install.packages("ROSE")
library(ROSE)

# Cargar datos
training_data <- read.csv("data/train.csv")

# Reemplazar valores NA con 0
training_data[is.na(training_data)] <- 0

mean_price <- mean(training_data$SalePrice, na.rm = TRUE)
sd_price <- sd(training_data$SalePrice, na.rm = TRUE)

# Definir los límites
lower_limit <- mean_price - (1 * sd_price)
upper_limit <- mean_price + (2 * sd_price)


# Crear la variable de clasificación
training_data$Category <- ifelse(training_data$SalePrice < lower_limit, "Baratas",
                           ifelse(training_data$SalePrice > upper_limit, "Caras", "Medianas"))
training_data <- training_data %>%
  mutate(
    EsBarata = ifelse(SalePrice < lower_limit, "Sí", "No"),
    EsCara = ifelse(SalePrice > upper_limit, "Sí", "No"),
    EsMediana = ifelse(SalePrice >= lower_limit & SalePrice <= upper_limit, "Sí", "No")
  )
training_data$Category <- as.factor(training_data$Category)
balanced_data <- ROSE(EsCara ~   OverallQual  + GarageCars + GrLivArea + Category, data = training_data)$data

predictors <- balanced_data %>% select(-Category)
response <- balanced_data$Category

# Normalización (Estandarización Z-score)
preProc <- preProcess(predictors, method = c("center", "scale"))
predictors_scaled <- predict(preProc, predictors)

# Semilla para reproducibilidad
set.seed(randomSeed) 

# Separar datos en entrenamiento (70%) y verificación (30%)
train_indices <- createDataPartition(response, p = 0.7, list = FALSE)

train_data <- predictors_scaled[train_indices, ]
test_data <- predictors_scaled[-train_indices, ]

library(caret)
library(e1071)

set.seed(randomSeed)  # Para reproducibilidad

control <- trainControl(method = "cv",    # cross-validation
                        number = 10,      # número de folds
                        classProbs = TRUE,  # para clasificación
                        summaryFunction = twoClassSummary)  # para métricas como ROC

# Puedes cambiar a Accuracy, Sensitivity, etc.
#modeloBarata <- train(EsBarata ~ LotArea + OverallQual + YearBuilt + GarageCars + GrLivArea,
#                   data = train_data,
#                   method = "glm",
#                   family = "binomial",
#                   trControl = control,
#                   metric = "ROC")
#modeloMediana <- train(EsBarata ~ LotArea + OverallQual + YearBuilt + GarageCars + GrLivArea,
#                   data = train_data,
#                   method = "glm",
#                   family = "binomial",
#                   trControl = control,
#                   metric = "ROC")
modeloMejoradoCara <- train(EsCara ~ OverallQual  + GarageCars + GrLivArea,
                   data = train_data,
                   method = "glm",
                   family = "binomial",
                   trControl = control,
                   metric = "Precision-Recall AUC")

#print("Modelo de EsBarata")
#print(modeloBarata)

#print("Modelo de EsMediana")
#print(modeloMediana)

print("Modelo de EsCara")

predicciones <- predict(modeloMejoradoCara, newdata = test_data)

# Convertir a clases (0 o 1) usando un umbral (por ejemplo, 0.5)
# predicciones <- ifelse(probabilidades > 0.5, 1, 0)
confusionMatrix(as.factor(predicciones), as.factor(test_data$EsCara))
```

Viendo la matriz de confusión obtenida vemos que estamos obteniendo más errores al predecir que una casa es cara, teniendo 22 casos (falsos negativos) que las casas si eran caras pero las clasifico como "No". Luego vemos que que hubo 10 casos (falsos positivos) que fueron mal clasificados. Entendemos que estos errores implican en que se afecta en la eficiencia de las ventas demostrando posibles falsos datos a los compradores haciendo que exista posibles pérdidas en las oportunidades de venta.

```{r time}
profvis({
  # Cargar datos
  training_data <- read.csv("data/train.csv")

  # Reemplazar valores NA con 0
  training_data[is.na(training_data)] <- 0

  mean_price <- mean(training_data$SalePrice, na.rm = TRUE)
  sd_price <- sd(training_data$SalePrice, na.rm = TRUE)

  # Definir los límites
  lower_limit <- mean_price - (1 * sd_price)
  upper_limit <- mean_price + (2 * sd_price)


  # Crear la variable de clasificación
  training_data <- training_data %>%
    mutate(
      EsBarata = ifelse(SalePrice < lower_limit, "Sí", "No"),
      EsCara = ifelse(SalePrice > upper_limit, "Sí", "No"),
      EsMediana = ifelse(SalePrice >= lower_limit & SalePrice <= upper_limit, "Sí", "No")
    )
  training_data$Category <- as.factor(ifelse(training_data$EsCara == "Sí", "Cara",
                                             ifelse(training_data$EsBarata == "Sí", "Barata", "Mediana")))
  balanced_data <- ROSE(EsCara ~ OverallQual + GarageCars + GrLivArea + Category, data = training_data)$data

  predictors <- balanced_data %>% select(-Category, -EsBarata, -EsMediana, -EsCara)
  response <- balanced_data$EsCara

  # Normalización (Estandarización Z-score)
  preProc <- preProcess(predictors, method = c("center", "scale"))
  predictors_scaled <- predict(preProc, predictors)

  # Semilla para reproducibilidad
  set.seed(randomSeed)

  # Separar datos en entrenamiento (70%) y verificación (30%)
  train_indices <- createDataPartition(response, p = 0.7, list = FALSE)

  train_data <- predictors_scaled[train_indices, ]
  test_data <- predictors_scaled[-train_indices, ]
  train_response <- response[train_indices]
  test_response <- response[-train_indices]

  set.seed(randomSeed)  # Para reproducibilidad

  control <- trainControl(method = "cv",    # cross-validation
                           number = 10,      # número de folds
                           classProbs = TRUE,  # para clasificación
                           summaryFunction = twoClassSummary)  # para métricas como ROC

  # Puedes cambiar a Accuracy, Sensitivity, etc.
  # modeloBarata <- train(EsBarata ~ LotArea + OverallQual + YearBuilt + GarageCars + GrLivArea,
  #                       data = train_data,
  #                       method = "glm",
  #                       family = "binomial",
  #                       trControl = control,
  #                       metric = "ROC")
  # modeloMediana <- train(EsBarata ~ LotArea + OverallQual + YearBuilt + GarageCars + GrLivArea,
  #                       data = train_data,
  #                       method = "glm",
  #                       family = "binomial",
  #                       trControl = control,
  #                       metric = "ROC")
  modeloMejoradoCara <- train(EsCara ~ OverallQual + GarageCars + GrLivArea,
                              data = train_data,
                              method = "glm",
                              family = "binomial",
                              trControl = control,
                              metric = "ROC") # Cambié la métrica a ROC para twoClassSummary

  # print("Modelo de EsBarata")
  # print(modeloBarata)

  # print("Modelo de EsMediana")
  # print(modeloMediana)

  print("Modelo de EsCara")
  print(modeloMejoradoCara)

  predicciones_prob <- predict(modeloMejoradoCara, newdata = test_data, type = "prob")
  predicciones <- ifelse(predicciones_prob$Sí > 0.5, "Sí", "No")

  confusionMatrix(as.factor(predicciones), as.factor(test_response))
})
```

En lo que respecta al tiempo y la memoria consumida se ve que para este modelo estos vaores son significativamente bajos. Se utilizo la libreria profviz para analizar estos datos y se obtuvo que el modelo consume un total de 17 MB que se asocia como una cantidad baja de memoria; por otro lado, se tiene un tiempo de ejecución total de 150 ms. La obtención de ambos valores bajos (memoria consumida y tiempo) indica una eficiencia algoritmica por parte del modelo mostrando una rápida y economica ejecución de este. 

Teniendo los datos anteriores seria bueno analizar los modelos implementados para determinar el mejor. Esto se puede conocer rápido al analizar las matrices de confusiones que se presentaron a lo largo del reporte. En este caso tenemos un modelo inicial y su versión mejorada, se tiene que el modelo mejorado presenta un mejor rendimiento lo que hace que sea este el mejor modelo.

## Comparación de modelos

Actualmente solo tenemos modelos que determinan si la una casa es cara o no, vamos a actualizarlo para deducir la variable "Categoría". A continuación se presenta la matriz de confusión obtenida por el modelo.

```{r categoric model}
library("nnet")
library("dplyr")

training_data <- read.csv("data/train.csv")
training_data[is.na(training_data)] <- 0

mean_price <- mean(training_data$SalePrice, na.rm = TRUE)
sd_price <- sd(training_data$SalePrice, na.rm = TRUE)

# Definir los límites
lower_limit <- mean_price - (1 * sd_price)
upper_limit <- mean_price + (2 * sd_price)


# Crear la variable de clasificación
training_data$Category <- ifelse(training_data$SalePrice < lower_limit, "Baratas",
                           ifelse(training_data$SalePrice > upper_limit, "Caras", "Medianas"))

training_data$Category <- as.factor(training_data$Category)

vars <- c("OverallQual", "GrLivArea", "YearBuilt", "TotalBsmtSF",
          "GarageCars", "GarageArea", "FullBath", "TotRmsAbvGrd")


set.seed(123)
trainIndex <- createDataPartition(training_data$Category, p = 0.8, list = FALSE)
train_set <- training_data[trainIndex, ]
test_set <- training_data[-trainIndex, ]

# Escalar variables numéricas
preproc <- preProcess(train_set[, vars], method = c("center", "scale"))
train_set[, vars] <- predict(preproc, train_set[, vars])
test_set[, vars] <- predict(preproc, test_set[, vars])

# Fórmula dinámica
formula <- as.formula(paste("Category ~", paste(vars, collapse = " + ")))

# Entrenar modelo multinomial
modelo <- multinom(formula, data = train_set)

# Predicciones
predicciones <- predict(modelo, newdata = test_set)

# Asegurar mismos niveles
predicciones <- factor(predicciones, levels = levels(test_set$Category))

confusionMatrix(predicciones, test_set$Category)
```

A continuación se presentaran las matrices de confusión de los modelos que se realizaron en reportes anteriores.

- KNN:

```{r Clasification Model}

# Calcular media y desviación estándar
mean_price <- mean(training_data$SalePrice, na.rm = TRUE)
sd_price <- sd(training_data$SalePrice, na.rm = TRUE)

# Definir umbrales
lower_threshold <- mean_price - 1.5 * sd_price
upper_threshold <- mean_price + 1.5 * sd_price

# Asignar categorías correctamente
training_data$SalePriceCategory <- case_when(
  training_data$SalePrice < lower_threshold ~ "Barata",
  training_data$SalePrice > upper_threshold ~ "Cara",
  TRUE ~ "Mediana"
)

# Convertir en factor
training_data$SalePriceCategory <- factor(training_data$SalePriceCategory)

# Partición de datos con la columna correcta
set.seed(123)
train_indices <- createDataPartition(training_data$SalePriceCategory, p = 0.7, list = FALSE)

train_data <- training_data[train_indices, ]
test_data <- training_data[-train_indices, ]
normalize <- function(x) { (x - min(x)) / (max(x) - min(x)) }

# Normalizar solo variables numéricas
train_data_numeric <- as.data.frame(lapply(train_data %>% select_if(is.numeric), normalize))
test_data_numeric <- as.data.frame(lapply(test_data %>% select_if(is.numeric), normalize))

# Definir variables predictoras y respuesta
train_labels <- train_data$SalePriceCategory
test_labels <- test_data$SalePriceCategory

# Definir número óptimo de vecinos
k <- round(sqrt(nrow(train_data)), 0)

# Aplicar KNN
knn_predictions <- knn(train = train_data_numeric,
                       test = test_data_numeric,
                       cl = train_labels,
                       k = k)
# Matriz de confusión
conf_matrix <- confusionMatrix(knn_predictions, test_labels)
print(conf_matrix)
```

- Naive Bayes:

```{r Naïve Bayes}
library(e1071)  # For Naïve Bayes
library(glmnet)

# Define price categories (Low, Medium, High) using quantiles
training_data$SalePriceCategory <- cut(training_data$SalePrice, 
                            breaks = quantile(training_data$SalePrice, probs = c(0, 0.33, 0.66, 1), na.rm = TRUE), 
                            labels = c("Barata", "Media", "Cara"), 
                            include.lowest = TRUE)

df <- training_data %>% select(-SalePrice, -Id)  # Remove ID and continuous SalePrice

df <- df %>% mutate_if(is.character, as.factor)

set.seed(42)
train_index <- createDataPartition(training_data$SalePriceCategory, p = 0.7, list = FALSE)
train_data <- df[train_index, ]
test_data <- df[-train_index, ]

nb_model <- naiveBayes(SalePriceCategory ~ ., data = train_data)

predictions <- predict(nb_model, test_data)

conf_matrix <- confusionMatrix(predictions, test_data$SalePriceCategory)
print(conf_matrix)
```

- Árbol de regresión:

```{r}
library(rpart)
library(caret)

set.seed(123)
splitIndex <- createDataPartition(training_data$Category, p = 0.8, list = FALSE)
train_data <- training_data[splitIndex, ]
test_data <- training_data[-splitIndex, ]
train_control <- trainControl(method = "cv", number = 10)

modelo_tree_cv <- train(Category ~ GrLivArea + OverallQual + GarageCars + YearBuilt,
                        data = train_data,
                        method = "rpart",
                        trControl = train_control,
                        tuneLength = 10)

predicciones <- predict(modelo_tree_cv, newdata = test_data)

confusionMatrix(predicciones, test_data$Category)

```

### Análisis de modelos

Al comparar las matrices de confusión para la predicción de la variable "Category", la Regresión Logística y el KNN exhiben las mayores exactitudes generales, ambas al rededor del 93-94%. Haciendo un analisis más profundo se ve la regresión logística presenta una mayor cooncordancia en sus datos presentados. El modelo de Naive Bayes presenta una excatitud significativamente menor (71.4%). El Árbol de Regresión se sitúa con una exactitud del 91.03%, quedando por debajo de los otros modelos mejores.

En lo que respecta a los errores presentado por cada modelo podemos ver lo siguiente. La Regresión Logística tiende a confundir las casas "Medianas" con "Baratas" y "Caras" en mayor medida. EL modelo de KNN muestra un patrón de error mayor al no predecir correctamente ningún caso de la categoría "Barata" y confundir significativamente las casas "Medianas" con "Caras". El Naive Bayes presenta errores distribuidos entre las clases, con una notable confusión entre "Medianas" y "Baratas". El Árbol de Regresión también muestra una tendecia a confundir "Medianas" con "Baratas" y "Caras".

En los tiempos de ejecución ningún modelo se tardo excesivamente mucho. El único modelo que tuvo una ejecución mas longeva fue el Árbol de regresión pero su tiempo agregado no se separa mucho del de los otros modelos. 

Teniendo todo el análisis anterior, tenemos que el modelo de regresión logística es el mejor modelo para predecir la variable "Category", ofreciendo un mejor equilibrio entre un alto rendimiento predictivo y una concordancia moderada.
---
title: "ReporteArboles"
author: "Flavio Galán, José Prince"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

load_data <- function(){
  training_data <- read.csv("data/train.csv")
  test_data <- read.csv("data/test.csv")
}

training_data <- read.csv("data/train.csv")
test_data <- read.csv("data/test.csv")
```

## Árbol multivariable
```{r Arbol multivariable}
# install.packages("rpart")       # Árboles de regresión
# install.packages("rpart.plot")  # Gráfico bonito del árbol

# Cargar librerías
library(rpart)
library(rpart.plot)

load_data()

# Crear el árbol de regresión
modelo_arbol <- rpart(SalePrice ~ ., data = training_data)

# Ver el resumen del árbol
summary(modelo_arbol)

# Gráfico simple
rpart.plot(modelo_arbol, type = 3, extra = 101, fallen.leaves = TRUE, cex = 0.7, main = "arbol de Regresion para SalePrice")

```

Ahora vamos a medir la precisión del modelo:
```{r Medir la precisión}
predicciones <- predict(modelo_arbol, training_data)

SSE <- sum((training_data$SalePrice - predicciones)^2)
SST <- sum((training_data$SalePrice - mean(training_data$SalePrice))^2)
R2 <- 1 - (SSE / SST)
cat("R-squared:", R2, "\n")

RMSE <- sqrt(mean((training_data$SalePrice - predicciones)^2))
cat("RMSE:", RMSE)

```

Este modelo predice la data con un 76% de variabilidad, además su RMSE es de 38 mil dólares lo cual sigue siendo alto aunque más bajo que el los lineales.

## Árboles de varios niveles

Ahora vamos a generar árboles con varios niveles de profundidad:
```{r Arboles de varios niveles}
# Cargar librerías
library(rpart)
library(rpart.plot)

load_data()

# Crear el árbol de regresión
arbol_2 <- rpart(SalePrice ~ ., data = training_data, control = rpart.control(maxdepth = 2))
arbol_3 <- rpart(SalePrice ~ ., data = training_data, control = rpart.control(maxdepth = 3))
arbol_8 <- rpart(SalePrice ~ ., data = training_data, control = rpart.control(maxdepth = 8))

# Ver el resumen del árbol
summary(modelo_arbol)

# Gráfico simple
rpart.plot(arbol_2, type = 3, extra = 101, fallen.leaves = TRUE, cex = 0.7, main = "arbol de Decision para SalePrice")
rpart.plot(arbol_3, type = 3, extra = 101, fallen.leaves = TRUE, cex = 0.7, main = "arbol de Decision para SalePrice")
rpart.plot(arbol_8, type = 3, extra = 101, fallen.leaves = TRUE, cex = 0.7, main = "arbol de Decision para SalePrice")
```
y para ver cuál es mejor lo comparamos con la data de entrenamiento:
```{r}

pre_tree <- function(tree, data, prefix) {

predicciones <- predict(modelo_arbol, training_data)

SSE <- sum((training_data$SalePrice - predicciones)^2)
SST <- sum((training_data$SalePrice - mean(training_data$SalePrice))^2)
R2 <- 1 - (SSE / SST)
cat(prefix, "\n")
cat("R-squared:", R2, "\n")

RMSE <- sqrt(mean((training_data$SalePrice - predicciones)^2))
cat("RMSE:", RMSE, "\n")
}

pre_tree(arbol_2, training_data, "Árbol de 2 Niveles")
pre_tree(arbol_3, training_data, "Árbol de 3 Niveles")
pre_tree(arbol_8, training_data, "Árbol de 8 Niveles")
```

Cómo se puede ver, todos los árboles predicen de igual forma los datos, incluso el que solamente tiene 2 niveles!

Los resultados del modelo de regresión lineal del ejercicio pasado solamente predicen a lo sumo el 69% de los datos, lo cual me dice que es menos preciso que los modelos de árboles de decisión, los cuáles predicen un 76.5% de los datos.

## Clasificación en 3 variables

Según el histograma de los datos, estos se encuentran altamente sesgados hacia la derecha:
```{r Histograma de SalesPrice}
library(ggplot2)

load_data()

mean_price <- mean(training_data$SalePrice, na.rm = TRUE)
sd_price <- sd(training_data$SalePrice, na.rm = TRUE)

# Definir los límites
lower_limit <- mean_price - (1 * sd_price)
upper_limit <- mean_price + (2 * sd_price)

ggplot(training_data, aes(x = SalePrice)) +
  geom_histogram(aes(y = ..density..), binwidth = 10000, fill = "blue", color = "black", alpha = 0.7) +
  geom_density(color = "red", size = 1.2) +
  geom_vline(xintercept = upper_limit, color = "purple")+
  geom_vline(xintercept = lower_limit, color = "purple")+
  geom_vline(xintercept = mean(training_data$SalePrice), color = "green")+
  theme_minimal() +
  labs(title = "Distribution of House Sale Prices with Density Curve",
       x = "Sale Price",
       y = "Density") +
  theme(plot.title = element_text(hjust = 0.5))
```
Como la distribución sí es normal entonces si podemos realizar las divisiones dependiendo de las desviaciones estándar. Por lo que definiremos las categorías como:

* Baratas: Todas las casas menores a media - 1.5 desviaciones estándar en valor.
* Medianas: Todas las casas entre media+- 1.5 desviaciones estándar en valor.
* Caras: Todas las casas mayores a media + 1.5 desviaciones estándar en valor.

```{r}
# Cargar librerías
library(rpart)       # Árboles de clasificación
library(rpart.plot)  # Visualización de árboles

# Crear la variable de clasificación
training_data$Category <- ifelse(training_data$SalePrice < lower_limit, "Baratas",
                           ifelse(training_data$SalePrice > upper_limit, "Caras", "Medianas"))
training_data$Category <- as.factor(training_data$Category)

# Revisamos la distribución
# table(training_data$Category)

modelo_arbol <- rpart(Category ~ GrLivArea + OverallQual + GarageCars + YearBuilt, 
                      data = training_data, 
                      method = "class", # Método de clasificación
                      control = rpart.control(minsplit = 30, cp = 0.01))

# Visualizamos el modelo
rpart.plot(modelo_arbol, type = 5, extra = 104, fallen.leaves = TRUE, cex = 0.8, box.palette = "RdYlGn")
```

